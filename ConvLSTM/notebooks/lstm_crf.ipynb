{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "import keras_contrib\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_contrib.layers import CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_pickle(\"../dataset/HAPT Data Set/dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(815614, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "\n",
    "    def __init__(self, dataset_root=None):\n",
    "        self.dataset_root = dataset_root\n",
    "        self.n_tags = 13\n",
    "        self.act_map = self.activity_map()\n",
    "    def activity_map(self):\n",
    "#         act_map = {'<IDLE>':0}\n",
    "        act_map = {}\n",
    "        with open(os.path.join(self.dataset_root, \"activity_labels.txt\"), \"r\") as al:\n",
    "            for line in al.readlines():\n",
    "                line = line.strip()\n",
    "                label, activity = line.split(\" \")\n",
    "                act_map[activity] = int(label)\n",
    "        return act_map   \n",
    "    def _load_data(self, file_path=None):\n",
    "        \"\"\"\n",
    "        Load train and test data into a (rows, columns) format numpy array.\n",
    "        \"\"\"\n",
    "        self.temp_df = pd.read_pickle(file_path)\n",
    "        self.temp_df['activity_id'] = self.temp_df['activity'].apply(lambda x: self.act_map[x]-1) \n",
    "        self.temp_df = self.temp_df.drop(columns = ['activity'])\n",
    "        \n",
    "    def load(self,split =\"train\"):\n",
    "        \"\"\"\n",
    "        Loads X and y.\n",
    "        \"\"\"\n",
    "        self._load_data(self.dataset_root+\"/\"+split+\".pkl\")\n",
    "        y = self.temp_df[self.temp_df.columns[-1]]\n",
    "        y = to_categorical(y)\n",
    "        X = self.temp_df[self.temp_df.columns[:7]]        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = Dataset(\"../dataset/HAPT Data Set/\")\n",
    "train_X,train_y = ds.load()\n",
    "test_X,test_y = ds.load(\"test\")\n",
    "train_y = pd.DataFrame(train_y)\n",
    "test_y = pd.DataFrame(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4    5    6    7    8    9    10   11\n",
       "0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "1  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "2  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "3  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "4  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp_number = len(list(set(train_X['exp_id'].values)))\n",
    "train_X_gp = train_X.groupby(['exp_id'])\n",
    "test_X_gp = test_X.groupby(['exp_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_dict = {}\n",
    "ct = 0\n",
    "for key, item in train_X_gp:\n",
    "    gp_df = pd.DataFrame(train_X_gp.get_group(key))\n",
    "#     print \"group:\"+str(key)\n",
    "#     print \"group_shape:\"+str(gp_df.shape[0])\n",
    "    ct += gp_df.shape[0]\n",
    "    del_row_idx = gp_df.shape[0]%128\n",
    "    group_dict[key] = (ct - del_row_idx,ct)\n",
    "#     print \"start_idx:\"+str(gp_df.shape[0] - del_row_idx)\n",
    "#     print \"end_idx:\"+str(gp_df.shape[0])\n",
    "#     trainx = pd.concat(trainx, gp_df)\n",
    "for g in group_dict:\n",
    "    start = group_dict[g][0]\n",
    "    end = group_dict[g][1]\n",
    "    if g in train_X['exp_id']:\n",
    "        train_X = train_X.drop(xrange(start, end)).copy(deep = True)\n",
    "        train_y = train_y.drop(xrange(start, end)).copy(deep = True)\n",
    "train_X.index = xrange(len(train_X))\n",
    "train_y.index = xrange(len(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_X = train_X.drop(columns = 'exp_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 563456 entries, 0 to 563455\n",
      "Data columns (total 12 columns):\n",
      "0     563456 non-null float32\n",
      "1     563456 non-null float32\n",
      "2     563456 non-null float32\n",
      "3     563456 non-null float32\n",
      "4     563456 non-null float32\n",
      "5     563456 non-null float32\n",
      "6     563456 non-null float32\n",
      "7     563456 non-null float32\n",
      "8     563456 non-null float32\n",
      "9     563456 non-null float32\n",
      "10    563456 non-null float32\n",
      "11    563456 non-null float32\n",
      "dtypes: float32(12)\n",
      "memory usage: 25.8 MB\n"
     ]
    }
   ],
   "source": [
    "train_y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tr_X.values\n",
    "X = np.reshape(X, newshape=(-1, 128, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4402, 128, 6)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_y.values\n",
    "y = np.reshape(y, newshape=(-1, 128, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4402, 128, 12)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = 0\n",
    "group_dict = {}\n",
    "for key, item in test_X_gp:\n",
    "    gp_df = pd.DataFrame(test_X_gp.get_group(key))\n",
    "#     print \"group:\"+str(key)\n",
    "#     print \"group_shape:\"+str(gp_df.shape[0])\n",
    "    ct += gp_df.shape[0]\n",
    "    del_row_idx = gp_df.shape[0]%128\n",
    "    group_dict[key] = (ct - del_row_idx,ct)\n",
    "for g in group_dict:\n",
    "    start = group_dict[g][0]\n",
    "    end = group_dict[g][1]\n",
    "    if g in test_X['exp_id']:\n",
    "        test_X = test_X.drop(xrange(start, end)).copy(deep = True)\n",
    "        test_y = test_y.drop(xrange(start, end)).copy(deep = True)\n",
    "test_X.index = xrange(len(test_X))\n",
    "test_y.index = xrange(len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_X = test_X.drop(columns = 'exp_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_X = np.reshape(te_X.values,newshape=(-1, 128, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_y = np.reshape(test_y.values,newshape = (-1,128,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1940, 128, 6), (1940, 128, 12))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te_X.shape, te_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>gyro_x</th>\n",
       "      <th>gyro_y</th>\n",
       "      <th>gyro_z</th>\n",
       "      <th>exp_id</th>\n",
       "      <th>usr_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LAYING</th>\n",
       "      <td>0.363873</td>\n",
       "      <td>0.461315</td>\n",
       "      <td>0.365683</td>\n",
       "      <td>-0.017025</td>\n",
       "      <td>0.014463</td>\n",
       "      <td>-0.053613</td>\n",
       "      <td>33.415950</td>\n",
       "      <td>16.596712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LIE_TO_SIT</th>\n",
       "      <td>0.086444</td>\n",
       "      <td>0.655496</td>\n",
       "      <td>0.584529</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.002084</td>\n",
       "      <td>0.007878</td>\n",
       "      <td>30.395157</td>\n",
       "      <td>15.121883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LIE_TO_STAND</th>\n",
       "      <td>0.079709</td>\n",
       "      <td>0.680099</td>\n",
       "      <td>0.528008</td>\n",
       "      <td>0.008728</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.004410</td>\n",
       "      <td>31.337444</td>\n",
       "      <td>15.594644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SITTING</th>\n",
       "      <td>0.832984</td>\n",
       "      <td>0.136460</td>\n",
       "      <td>0.170461</td>\n",
       "      <td>0.030081</td>\n",
       "      <td>-0.006273</td>\n",
       "      <td>0.019097</td>\n",
       "      <td>33.457621</td>\n",
       "      <td>16.624999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIT_TO_LIE</th>\n",
       "      <td>0.999628</td>\n",
       "      <td>0.047424</td>\n",
       "      <td>0.052002</td>\n",
       "      <td>-0.006552</td>\n",
       "      <td>-0.011597</td>\n",
       "      <td>-0.003875</td>\n",
       "      <td>32.061957</td>\n",
       "      <td>15.943836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIT_TO_STAND</th>\n",
       "      <td>0.905752</td>\n",
       "      <td>0.240520</td>\n",
       "      <td>0.228301</td>\n",
       "      <td>0.009407</td>\n",
       "      <td>0.011619</td>\n",
       "      <td>-0.008985</td>\n",
       "      <td>31.556856</td>\n",
       "      <td>15.678291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STANDING</th>\n",
       "      <td>0.935457</td>\n",
       "      <td>-0.102094</td>\n",
       "      <td>0.088819</td>\n",
       "      <td>-0.013834</td>\n",
       "      <td>-0.023273</td>\n",
       "      <td>0.012171</td>\n",
       "      <td>33.096818</td>\n",
       "      <td>16.441222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STAND_TO_LIE</th>\n",
       "      <td>0.990244</td>\n",
       "      <td>-0.177374</td>\n",
       "      <td>-0.050188</td>\n",
       "      <td>0.062486</td>\n",
       "      <td>-0.023687</td>\n",
       "      <td>-0.014757</td>\n",
       "      <td>29.484811</td>\n",
       "      <td>14.683035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STAND_TO_SIT</th>\n",
       "      <td>1.005577</td>\n",
       "      <td>-0.153504</td>\n",
       "      <td>0.010305</td>\n",
       "      <td>-0.001116</td>\n",
       "      <td>-0.000990</td>\n",
       "      <td>0.006703</td>\n",
       "      <td>31.725960</td>\n",
       "      <td>15.760372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WALKING</th>\n",
       "      <td>0.976176</td>\n",
       "      <td>-0.160281</td>\n",
       "      <td>-0.044960</td>\n",
       "      <td>0.010646</td>\n",
       "      <td>-0.005910</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>30.656691</td>\n",
       "      <td>15.248610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WALKING_DOWNSTAIRS</th>\n",
       "      <td>0.980216</td>\n",
       "      <td>-0.193211</td>\n",
       "      <td>-0.089862</td>\n",
       "      <td>0.064939</td>\n",
       "      <td>-0.012897</td>\n",
       "      <td>-0.022317</td>\n",
       "      <td>31.732653</td>\n",
       "      <td>15.775771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WALKING_UPSTAIRS</th>\n",
       "      <td>0.981021</td>\n",
       "      <td>-0.214094</td>\n",
       "      <td>-0.102791</td>\n",
       "      <td>0.012116</td>\n",
       "      <td>-0.018525</td>\n",
       "      <td>-0.001478</td>\n",
       "      <td>31.464462</td>\n",
       "      <td>15.644606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       acc_x     acc_y     acc_z    gyro_x    gyro_y  \\\n",
       "activity                                                               \n",
       "LAYING              0.363873  0.461315  0.365683 -0.017025  0.014463   \n",
       "LIE_TO_SIT          0.086444  0.655496  0.584529  0.007800  0.002084   \n",
       "LIE_TO_STAND        0.079709  0.680099  0.528008  0.008728  0.001275   \n",
       "SITTING             0.832984  0.136460  0.170461  0.030081 -0.006273   \n",
       "SIT_TO_LIE          0.999628  0.047424  0.052002 -0.006552 -0.011597   \n",
       "SIT_TO_STAND        0.905752  0.240520  0.228301  0.009407  0.011619   \n",
       "STANDING            0.935457 -0.102094  0.088819 -0.013834 -0.023273   \n",
       "STAND_TO_LIE        0.990244 -0.177374 -0.050188  0.062486 -0.023687   \n",
       "STAND_TO_SIT        1.005577 -0.153504  0.010305 -0.001116 -0.000990   \n",
       "WALKING             0.976176 -0.160281 -0.044960  0.010646 -0.005910   \n",
       "WALKING_DOWNSTAIRS  0.980216 -0.193211 -0.089862  0.064939 -0.012897   \n",
       "WALKING_UPSTAIRS    0.981021 -0.214094 -0.102791  0.012116 -0.018525   \n",
       "\n",
       "                      gyro_z     exp_id     usr_id  \n",
       "activity                                            \n",
       "LAYING             -0.053613  33.415950  16.596712  \n",
       "LIE_TO_SIT          0.007878  30.395157  15.121883  \n",
       "LIE_TO_STAND        0.004410  31.337444  15.594644  \n",
       "SITTING             0.019097  33.457621  16.624999  \n",
       "SIT_TO_LIE         -0.003875  32.061957  15.943836  \n",
       "SIT_TO_STAND       -0.008985  31.556856  15.678291  \n",
       "STANDING            0.012171  33.096818  16.441222  \n",
       "STAND_TO_LIE       -0.014757  29.484811  14.683035  \n",
       "STAND_TO_SIT        0.006703  31.725960  15.760372  \n",
       "WALKING             0.002779  30.656691  15.248610  \n",
       "WALKING_DOWNSTAIRS -0.022317  31.732653  15.775771  \n",
       "WALKING_UPSTAIRS   -0.001478  31.464462  15.644606  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.groupby(\"activity\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 128, 6)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 128, 100)          22800     \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 128, 200)          160800    \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 128, 50)           10050     \n",
      "_________________________________________________________________\n",
      "crf_7 (CRF)                  (None, 128, 12)           780       \n",
      "=================================================================\n",
      "Total params: 194,430\n",
      "Trainable params: 194,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(128, 6))\n",
    "model = Bidirectional(LSTM(units=50, return_sequences=True,\n",
    "                           recurrent_dropout=0.1))(input)  # variational biLSTM\n",
    "model = Bidirectional(LSTM(units=100, return_sequences=True,\n",
    "                           recurrent_dropout=0.1))(model)  # variational biLSTM\n",
    "model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "crf = CRF(12)  # CRF layer\n",
    "out = crf(model)  # output\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"nadam\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3961 samples, validate on 441 samples\n",
      "Epoch 1/50\n",
      "3961/3961 [==============================] - 38s 10ms/step - loss: 1.3977 - viterbi_acc: 0.3703 - val_loss: 1.2551 - val_viterbi_acc: 0.3743\n",
      "Epoch 2/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: 0.9163 - viterbi_acc: 0.4793 - val_loss: 0.9572 - val_viterbi_acc: 0.3964\n",
      "Epoch 3/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: 0.6966 - viterbi_acc: 0.5146 - val_loss: 0.9430 - val_viterbi_acc: 0.3790\n",
      "Epoch 4/50\n",
      "3961/3961 [==============================] - 40s 10ms/step - loss: 0.5425 - viterbi_acc: 0.5307 - val_loss: 0.6860 - val_viterbi_acc: 0.4363\n",
      "Epoch 5/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: 0.4307 - viterbi_acc: 0.5521 - val_loss: 0.6016 - val_viterbi_acc: 0.4156\n",
      "Epoch 6/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: 0.3495 - viterbi_acc: 0.5687 - val_loss: 0.5128 - val_viterbi_acc: 0.4773\n",
      "Epoch 7/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: 0.2845 - viterbi_acc: 0.5840 - val_loss: 0.4268 - val_viterbi_acc: 0.4897\n",
      "Epoch 8/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: 0.2377 - viterbi_acc: 0.5979 - val_loss: 0.3796 - val_viterbi_acc: 0.4447\n",
      "Epoch 9/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: 0.2025 - viterbi_acc: 0.6139 - val_loss: 0.3702 - val_viterbi_acc: 0.4757\n",
      "Epoch 10/50\n",
      "3961/3961 [==============================] - 35s 9ms/step - loss: 0.1712 - viterbi_acc: 0.6218 - val_loss: 0.3484 - val_viterbi_acc: 0.4447\n",
      "Epoch 11/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: 0.1495 - viterbi_acc: 0.6315 - val_loss: 0.2990 - val_viterbi_acc: 0.5234\n",
      "Epoch 12/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: 0.1268 - viterbi_acc: 0.6474 - val_loss: 0.2765 - val_viterbi_acc: 0.5000\n",
      "Epoch 13/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: 0.1130 - viterbi_acc: 0.6466 - val_loss: 0.2517 - val_viterbi_acc: 0.5104\n",
      "Epoch 14/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: 0.0994 - viterbi_acc: 0.6516 - val_loss: 0.2320 - val_viterbi_acc: 0.4938\n",
      "Epoch 15/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: 0.0916 - viterbi_acc: 0.6497 - val_loss: 0.2130 - val_viterbi_acc: 0.5004\n",
      "Epoch 16/50\n",
      "3961/3961 [==============================] - 41s 10ms/step - loss: 0.0767 - viterbi_acc: 0.6721 - val_loss: 0.2025 - val_viterbi_acc: 0.4969\n",
      "Epoch 17/50\n",
      "3961/3961 [==============================] - 42s 11ms/step - loss: 0.0680 - viterbi_acc: 0.6702 - val_loss: 0.2006 - val_viterbi_acc: 0.5007\n",
      "Epoch 18/50\n",
      "3961/3961 [==============================] - 43s 11ms/step - loss: 0.0614 - viterbi_acc: 0.6691 - val_loss: 0.1907 - val_viterbi_acc: 0.5060\n",
      "Epoch 19/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: 0.0557 - viterbi_acc: 0.6834 - val_loss: 0.1750 - val_viterbi_acc: 0.5120\n",
      "Epoch 20/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: 0.0487 - viterbi_acc: 0.6890 - val_loss: 0.1727 - val_viterbi_acc: 0.5031\n",
      "Epoch 21/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: 0.0426 - viterbi_acc: 0.6940 - val_loss: 0.1565 - val_viterbi_acc: 0.4903\n",
      "Epoch 22/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: 0.0377 - viterbi_acc: 0.6948 - val_loss: 0.1479 - val_viterbi_acc: 0.5044\n",
      "Epoch 23/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: 0.0338 - viterbi_acc: 0.6988 - val_loss: 0.1357 - val_viterbi_acc: 0.5132\n",
      "Epoch 24/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: 0.0311 - viterbi_acc: 0.6976 - val_loss: 0.1145 - val_viterbi_acc: 0.5121\n",
      "Epoch 25/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: 0.0267 - viterbi_acc: 0.6986 - val_loss: 0.1339 - val_viterbi_acc: 0.5181\n",
      "Epoch 26/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: 0.0230 - viterbi_acc: 0.7102 - val_loss: 0.1202 - val_viterbi_acc: 0.4884\n",
      "Epoch 27/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: 0.0206 - viterbi_acc: 0.7149 - val_loss: 0.1099 - val_viterbi_acc: 0.4883\n",
      "Epoch 28/50\n",
      "3961/3961 [==============================] - 38s 10ms/step - loss: 0.0174 - viterbi_acc: 0.7176 - val_loss: 0.1078 - val_viterbi_acc: 0.4889\n",
      "Epoch 29/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: 0.0154 - viterbi_acc: 0.7189 - val_loss: 0.1054 - val_viterbi_acc: 0.5104\n",
      "Epoch 30/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: 0.0131 - viterbi_acc: 0.7203 - val_loss: 0.0967 - val_viterbi_acc: 0.5391\n",
      "Epoch 31/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: 0.0104 - viterbi_acc: 0.7187 - val_loss: 0.0995 - val_viterbi_acc: 0.5019\n",
      "Epoch 32/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: 0.0088 - viterbi_acc: 0.7220 - val_loss: 0.0868 - val_viterbi_acc: 0.5010\n",
      "Epoch 33/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: 0.0058 - viterbi_acc: 0.7226 - val_loss: 0.0843 - val_viterbi_acc: 0.5199\n",
      "Epoch 34/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: 0.0033 - viterbi_acc: 0.7393 - val_loss: 0.0743 - val_viterbi_acc: 0.5282\n",
      "Epoch 35/50\n",
      "3961/3961 [==============================] - 38s 10ms/step - loss: 0.0015 - viterbi_acc: 0.7325 - val_loss: 0.0657 - val_viterbi_acc: 0.5132\n",
      "Epoch 36/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: -4.3056e-04 - viterbi_acc: 0.7397 - val_loss: 0.0640 - val_viterbi_acc: 0.4996\n",
      "Epoch 37/50\n",
      "3961/3961 [==============================] - 40s 10ms/step - loss: -0.0022 - viterbi_acc: 0.7288 - val_loss: 0.0679 - val_viterbi_acc: 0.4971\n",
      "Epoch 38/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0043 - viterbi_acc: 0.7353 - val_loss: 0.0570 - val_viterbi_acc: 0.4507\n",
      "Epoch 39/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: -0.0063 - viterbi_acc: 0.7409 - val_loss: 0.0604 - val_viterbi_acc: 0.5351\n",
      "Epoch 40/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: -0.0080 - viterbi_acc: 0.7369 - val_loss: 0.0482 - val_viterbi_acc: 0.4017\n",
      "Epoch 41/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0102 - viterbi_acc: 0.7398 - val_loss: 0.0487 - val_viterbi_acc: 0.5298\n",
      "Epoch 42/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: -0.0121 - viterbi_acc: 0.7444 - val_loss: 0.0385 - val_viterbi_acc: 0.5058\n",
      "Epoch 43/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0144 - viterbi_acc: 0.7507 - val_loss: 0.0444 - val_viterbi_acc: 0.4965\n",
      "Epoch 44/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0160 - viterbi_acc: 0.7514 - val_loss: 0.0428 - val_viterbi_acc: 0.5243\n",
      "Epoch 45/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0181 - viterbi_acc: 0.7593 - val_loss: 0.0407 - val_viterbi_acc: 0.5196\n",
      "Epoch 46/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: -0.0199 - viterbi_acc: 0.7588 - val_loss: 0.0373 - val_viterbi_acc: 0.5147\n",
      "Epoch 47/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0212 - viterbi_acc: 0.7624 - val_loss: 0.0309 - val_viterbi_acc: 0.4992\n",
      "Epoch 48/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0216 - viterbi_acc: 0.7435 - val_loss: 0.0329 - val_viterbi_acc: 0.4715\n",
      "Epoch 49/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0239 - viterbi_acc: 0.7495 - val_loss: 0.0265 - val_viterbi_acc: 0.4777\n",
      "Epoch 50/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0261 - viterbi_acc: 0.7593 - val_loss: 0.0331 - val_viterbi_acc: 0.5068\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, np.array(y), batch_size=32, epochs=50,\n",
    "                    validation_split=0.1, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3961 samples, validate on 441 samples\n",
      "Epoch 1/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: -0.0282 - viterbi_acc: 0.7626 - val_loss: 0.0232 - val_viterbi_acc: 0.5156\n",
      "Epoch 2/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0291 - viterbi_acc: 0.7571 - val_loss: 0.0227 - val_viterbi_acc: 0.5006\n",
      "Epoch 3/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0309 - viterbi_acc: 0.7633 - val_loss: 0.0208 - val_viterbi_acc: 0.5024\n",
      "Epoch 4/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: -0.0327 - viterbi_acc: 0.7631 - val_loss: 0.0141 - val_viterbi_acc: 0.5132\n",
      "Epoch 5/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0347 - viterbi_acc: 0.7716 - val_loss: 0.0140 - val_viterbi_acc: 0.4372\n",
      "Epoch 6/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0366 - viterbi_acc: 0.7731 - val_loss: 0.0103 - val_viterbi_acc: 0.5154\n",
      "Epoch 7/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0382 - viterbi_acc: 0.7800 - val_loss: 0.0184 - val_viterbi_acc: 0.4874\n",
      "Epoch 8/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: -0.0388 - viterbi_acc: 0.7633 - val_loss: 0.0037 - val_viterbi_acc: 0.4979\n",
      "Epoch 9/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: -0.0411 - viterbi_acc: 0.7719 - val_loss: 0.0031 - val_viterbi_acc: 0.4914\n",
      "Epoch 10/50\n",
      "3961/3961 [==============================] - 38s 10ms/step - loss: -0.0419 - viterbi_acc: 0.7674 - val_loss: -0.0034 - val_viterbi_acc: 0.5139\n",
      "Epoch 11/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: -0.0447 - viterbi_acc: 0.7821 - val_loss: -0.0021 - val_viterbi_acc: 0.4671\n",
      "Epoch 12/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0464 - viterbi_acc: 0.7769 - val_loss: 0.0017 - val_viterbi_acc: 0.5221\n",
      "Epoch 13/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: -0.0478 - viterbi_acc: 0.7820 - val_loss: -0.0014 - val_viterbi_acc: 0.4168\n",
      "Epoch 14/50\n",
      "3961/3961 [==============================] - 38s 10ms/step - loss: -0.0495 - viterbi_acc: 0.7788 - val_loss: -0.0058 - val_viterbi_acc: 0.4916\n",
      "Epoch 15/50\n",
      "3961/3961 [==============================] - 38s 9ms/step - loss: -0.0511 - viterbi_acc: 0.7845 - val_loss: -0.0029 - val_viterbi_acc: 0.5165\n",
      "Epoch 16/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0522 - viterbi_acc: 0.7690 - val_loss: -0.0150 - val_viterbi_acc: 0.4963\n",
      "Epoch 17/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0534 - viterbi_acc: 0.7687 - val_loss: -0.0139 - val_viterbi_acc: 0.5179\n",
      "Epoch 18/50\n",
      "3961/3961 [==============================] - 38s 10ms/step - loss: -0.0557 - viterbi_acc: 0.7827 - val_loss: -0.0171 - val_viterbi_acc: 0.4878\n",
      "Epoch 19/50\n",
      "3961/3961 [==============================] - 38s 10ms/step - loss: -0.0580 - viterbi_acc: 0.7955 - val_loss: -0.0114 - val_viterbi_acc: 0.4957\n",
      "Epoch 20/50\n",
      "3961/3961 [==============================] - 39s 10ms/step - loss: -0.0596 - viterbi_acc: 0.7953 - val_loss: -0.0143 - val_viterbi_acc: 0.5195\n",
      "Epoch 21/50\n",
      "3961/3961 [==============================] - 38s 10ms/step - loss: -0.0608 - viterbi_acc: 0.7878 - val_loss: -0.0179 - val_viterbi_acc: 0.5025\n",
      "Epoch 22/50\n",
      "3961/3961 [==============================] - 41s 10ms/step - loss: -0.0618 - viterbi_acc: 0.7705 - val_loss: -0.0177 - val_viterbi_acc: 0.5068\n",
      "Epoch 23/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0642 - viterbi_acc: 0.7887 - val_loss: -0.0187 - val_viterbi_acc: 0.4861\n",
      "Epoch 24/50\n",
      "3961/3961 [==============================] - 39s 10ms/step - loss: -0.0660 - viterbi_acc: 0.7953 - val_loss: -0.0237 - val_viterbi_acc: 0.5142\n",
      "Epoch 25/50\n",
      "3961/3961 [==============================] - 41s 10ms/step - loss: -0.0668 - viterbi_acc: 0.7818 - val_loss: -0.0272 - val_viterbi_acc: 0.4993\n",
      "Epoch 26/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0690 - viterbi_acc: 0.7907 - val_loss: -0.0272 - val_viterbi_acc: 0.5012\n",
      "Epoch 27/50\n",
      "3961/3961 [==============================] - 39s 10ms/step - loss: -0.0706 - viterbi_acc: 0.8036 - val_loss: -0.0253 - val_viterbi_acc: 0.5356\n",
      "Epoch 28/50\n",
      "3961/3961 [==============================] - 38s 10ms/step - loss: -0.0723 - viterbi_acc: 0.7975 - val_loss: -0.0307 - val_viterbi_acc: 0.5018\n",
      "Epoch 29/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0735 - viterbi_acc: 0.7918 - val_loss: -0.0317 - val_viterbi_acc: 0.4963\n",
      "Epoch 30/50\n",
      "3961/3961 [==============================] - 40s 10ms/step - loss: -0.0755 - viterbi_acc: 0.7957 - val_loss: -0.0340 - val_viterbi_acc: 0.5159\n",
      "Epoch 31/50\n",
      "3961/3961 [==============================] - 41s 10ms/step - loss: -0.0771 - viterbi_acc: 0.7995 - val_loss: -0.0349 - val_viterbi_acc: 0.5082\n",
      "Epoch 32/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0787 - viterbi_acc: 0.8000 - val_loss: -0.0376 - val_viterbi_acc: 0.5155\n",
      "Epoch 33/50\n",
      "3961/3961 [==============================] - 39s 10ms/step - loss: -0.0802 - viterbi_acc: 0.7952 - val_loss: -0.0354 - val_viterbi_acc: 0.5098\n",
      "Epoch 34/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0815 - viterbi_acc: 0.7840 - val_loss: -0.0415 - val_viterbi_acc: 0.5096\n",
      "Epoch 35/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0829 - viterbi_acc: 0.7940 - val_loss: -0.0457 - val_viterbi_acc: 0.5159\n",
      "Epoch 36/50\n",
      "3961/3961 [==============================] - 41s 10ms/step - loss: -0.0855 - viterbi_acc: 0.8076 - val_loss: -0.0333 - val_viterbi_acc: 0.5156\n",
      "Epoch 37/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: -0.0868 - viterbi_acc: 0.8081 - val_loss: -0.0427 - val_viterbi_acc: 0.4709\n",
      "Epoch 38/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0888 - viterbi_acc: 0.8154 - val_loss: -0.0394 - val_viterbi_acc: 0.5055\n",
      "Epoch 39/50\n",
      "3961/3961 [==============================] - 35s 9ms/step - loss: -0.0899 - viterbi_acc: 0.8055 - val_loss: -0.0436 - val_viterbi_acc: 0.4774\n",
      "Epoch 40/50\n",
      "3961/3961 [==============================] - 35s 9ms/step - loss: -0.0913 - viterbi_acc: 0.8018 - val_loss: -0.0425 - val_viterbi_acc: 0.4964\n",
      "Epoch 41/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0929 - viterbi_acc: 0.8089 - val_loss: -0.0498 - val_viterbi_acc: 0.4912\n",
      "Epoch 42/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: -0.0945 - viterbi_acc: 0.8046 - val_loss: -0.0471 - val_viterbi_acc: 0.5058\n",
      "Epoch 43/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: -0.0957 - viterbi_acc: 0.7945 - val_loss: -0.0517 - val_viterbi_acc: 0.5172\n",
      "Epoch 44/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: -0.0975 - viterbi_acc: 0.8054 - val_loss: -0.0523 - val_viterbi_acc: 0.4871\n",
      "Epoch 45/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.0996 - viterbi_acc: 0.8156 - val_loss: -0.0525 - val_viterbi_acc: 0.5122\n",
      "Epoch 46/50\n",
      "3961/3961 [==============================] - 35s 9ms/step - loss: -0.1012 - viterbi_acc: 0.8197 - val_loss: -0.0499 - val_viterbi_acc: 0.5153\n",
      "Epoch 47/50\n",
      "3961/3961 [==============================] - 37s 9ms/step - loss: -0.1024 - viterbi_acc: 0.8072 - val_loss: -0.0530 - val_viterbi_acc: 0.5250\n",
      "Epoch 48/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: -0.1036 - viterbi_acc: 0.8030 - val_loss: -0.0596 - val_viterbi_acc: 0.5168\n",
      "Epoch 49/50\n",
      "3961/3961 [==============================] - 36s 9ms/step - loss: -0.1058 - viterbi_acc: 0.8191 - val_loss: -0.0571 - val_viterbi_acc: 0.4948\n",
      "Epoch 50/50\n",
      "3961/3961 [==============================] - 38s 10ms/step - loss: -0.1075 - viterbi_acc: 0.8208 - val_loss: -0.0554 - val_viterbi_acc: 0.5274\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, np.array(y), batch_size=32, epochs=50,\n",
    "                    validation_split=0.1, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1940/1940 [==============================] - 5s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.08320342098775599, 0.6165673323513307]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(te_X,te_y,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('82model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM CRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM_model(Model):\n",
    "\n",
    "    def __init__(self, train_data=None, test_data=None, tb_log_dir=None):\n",
    "        self.n_timesteps = 128\n",
    "        self.n_features = 0\n",
    "        self.build()\n",
    "\n",
    "    def evaluate(self, log_dir=None):\n",
    "        accuracy = super().evaluate()\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def build(self):\n",
    "        model = Input(shape=(self.n_timesteps,self.n_features))       \n",
    "        model = Bidirectional(LSTM(units=50, return_sequences=True,\n",
    "                           recurrent_dropout=0.1))(model)  # variational biLSTM\n",
    "        model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "        crf = CRF()\n",
    "#         model.add(LSTM(100, return_Sequences = True, input_shape=(\n",
    "#             self.n_timesteps, self.n_features)))\n",
    "#         model.add(Dropout(0.5))\n",
    "#         model.add(Dense(100, activation='relu'))\n",
    "#         model.add(Dense(self.n_outputs, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                           optimizer='adam', metrics=['accuracy'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
